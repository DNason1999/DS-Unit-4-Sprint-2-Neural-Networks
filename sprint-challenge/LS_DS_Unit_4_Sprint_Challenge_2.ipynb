{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** A single point in a neural network that contains an associated weight and an activation function to determine its output\n",
    "- **Input Layer:** The first layer of a neural network that recieves the data from the user and passes it on to the first hidden layer or output layer\n",
    "- **Hidden Layer:** Any layer between the input and output layers, which contains neurons, weights, and biases\n",
    "- **Output Layer:** The final layer of the neural network which performs the final feedforward and creates an output\n",
    "- **Activation:** The function that determines the output of a neuron based on the dot product of the input to the neuron and the associated weight\n",
    "- **Backpropagation:** The act of analyzing the error produced in a supervised neural network from the predicted result and the given result \n",
    "                      and changing the weights in the neural network to hopefully decrease the error for future predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  candy[['chocolate','gummy']]\n",
    "target = candy[['ate']]\n",
    "\n",
    "X_test, X_train = features[:-(len(features)//8)*7], features[-(len(features)//8)*7:]\n",
    "y_test, y_train = target[:-(len(features)//8)*7], target[-(len(features)//8)*7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "class Perceptron():\n",
    "    def __init__(self, numIn, numOut, iterations):\n",
    "        self.weights = 2 * np.random.random((numIn,numOut)) - 1\n",
    "        self.bias = np.random.random((1,1))[0]\n",
    "        self.iter = iterations\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_prime(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def forward_backward(self, x, y):\n",
    "        weighted_sum = np.dot(x, self.weights) + self.bias\n",
    "        \n",
    "        self.activated = self.sigmoid(weighted_sum)\n",
    "    \n",
    "        error = y - self.activated\n",
    "        \n",
    "        self.weights += np.dot(x.T, error * self.sigmoid_prime(self.activated))\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        for i in range(0,self.iter):\n",
    "            self.forward_backward(x,y)\n",
    "            print('Iteration #: {}'.format(i))\n",
    "            \n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.weights)\n",
    "        return np.where(z >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(2, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #: 0\n",
      "Iteration #: 1\n",
      "Iteration #: 2\n",
      "Iteration #: 3\n",
      "Iteration #: 4\n",
      "Iteration #: 5\n",
      "Iteration #: 6\n",
      "Iteration #: 7\n",
      "Iteration #: 8\n",
      "Iteration #: 9\n",
      "Iteration #: 10\n",
      "Iteration #: 11\n",
      "Iteration #: 12\n",
      "Iteration #: 13\n",
      "Iteration #: 14\n",
      "Iteration #: 15\n",
      "Iteration #: 16\n",
      "Iteration #: 17\n",
      "Iteration #: 18\n",
      "Iteration #: 19\n",
      "Iteration #: 20\n",
      "Iteration #: 21\n",
      "Iteration #: 22\n",
      "Iteration #: 23\n",
      "Iteration #: 24\n",
      "Iteration #: 25\n",
      "Iteration #: 26\n",
      "Iteration #: 27\n",
      "Iteration #: 28\n",
      "Iteration #: 29\n",
      "Iteration #: 30\n",
      "Iteration #: 31\n",
      "Iteration #: 32\n",
      "Iteration #: 33\n",
      "Iteration #: 34\n",
      "Iteration #: 35\n",
      "Iteration #: 36\n",
      "Iteration #: 37\n",
      "Iteration #: 38\n",
      "Iteration #: 39\n",
      "Iteration #: 40\n",
      "Iteration #: 41\n",
      "Iteration #: 42\n",
      "Iteration #: 43\n",
      "Iteration #: 44\n",
      "Iteration #: 45\n",
      "Iteration #: 46\n",
      "Iteration #: 47\n",
      "Iteration #: 48\n",
      "Iteration #: 49\n",
      "Iteration #: 50\n",
      "Iteration #: 51\n",
      "Iteration #: 52\n",
      "Iteration #: 53\n",
      "Iteration #: 54\n",
      "Iteration #: 55\n",
      "Iteration #: 56\n",
      "Iteration #: 57\n",
      "Iteration #: 58\n",
      "Iteration #: 59\n",
      "Iteration #: 60\n",
      "Iteration #: 61\n",
      "Iteration #: 62\n",
      "Iteration #: 63\n",
      "Iteration #: 64\n",
      "Iteration #: 65\n",
      "Iteration #: 66\n",
      "Iteration #: 67\n",
      "Iteration #: 68\n",
      "Iteration #: 69\n",
      "Iteration #: 70\n",
      "Iteration #: 71\n",
      "Iteration #: 72\n",
      "Iteration #: 73\n",
      "Iteration #: 74\n",
      "Iteration #: 75\n",
      "Iteration #: 76\n",
      "Iteration #: 77\n",
      "Iteration #: 78\n",
      "Iteration #: 79\n",
      "Iteration #: 80\n",
      "Iteration #: 81\n",
      "Iteration #: 82\n",
      "Iteration #: 83\n",
      "Iteration #: 84\n",
      "Iteration #: 85\n",
      "Iteration #: 86\n",
      "Iteration #: 87\n",
      "Iteration #: 88\n",
      "Iteration #: 89\n",
      "Iteration #: 90\n",
      "Iteration #: 91\n",
      "Iteration #: 92\n",
      "Iteration #: 93\n",
      "Iteration #: 94\n",
      "Iteration #: 95\n",
      "Iteration #: 96\n",
      "Iteration #: 97\n",
      "Iteration #: 98\n",
      "Iteration #: 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dylan nason\\anaconda3\\envs\\u4-s2-nn\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "p.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = p.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5104"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why a simple Perceptron cannot get higher than ~50% accuracy\n",
    "The problem is that the data resembles an XOR gate, which is not linearly seperable. For a simple perceptron to make accurate\n",
    "predictions, the problem must be linearly seperable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class MultiPerceptron:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Multilayer Perceptron\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 2\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x4 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 4x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        sx = self.sigmoid(s)\n",
    "        return sx * (1 - sx)\n",
    "    \n",
    "    def feed_forward(self, X):        \n",
    "        # Weighted sum between inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "    \n",
    "    def backward(self, X,y,o):\n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        o = self.feed_forward(X)\n",
    "        return np.where(o >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MultiPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dylan nason\\anaconda3\\envs\\u4-s2-nn\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "for x in range(0,100):\n",
    "    mp.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dylan nason\\anaconda3\\envs\\u4-s2-nn\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "predictions_mp = mp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7264"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_mp = accuracy_score(y_test, predictions_mp)\n",
    "acc_mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why multilayer perceptron did better than simple perceptron\n",
    "The multilayer perceptron is able to perform better on a non-linearly seperable task because it is able to perform more than a single operation to create an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>226</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "102   63    0   1       140   195    0        1      179      0      0.0   \n",
       "148   44    1   2       120   226    0        1      169      0      0.0   \n",
       "188   50    1   2       140   233    0        1      163      0      0.6   \n",
       "290   61    1   0       148   203    0        1      161      0      0.0   \n",
       "227   35    1   0       120   198    0        1      130      1      1.6   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "102      2   2     2       1  \n",
       "148      2   0     2       1  \n",
       "188      1   1     3       0  \n",
       "290      2   1     3       0  \n",
       "227      1   0     3       0  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[df.columns[:-1]]\n",
    "target = df[['target']]\n",
    "\n",
    "X_test, X_train = features[:-(len(features)//8)*7], features[-(len(features)//8)*7:]\n",
    "y_test, y_train = target[:-(len(features)//8)*7], target[-(len(features)//8)*7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 13)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "259/259 [==============================] - 0s 788us/sample - loss: 41.3143 - acc: 0.4440\n",
      "Epoch 2/100\n",
      "259/259 [==============================] - 0s 89us/sample - loss: 30.0060 - acc: 0.4440\n",
      "Epoch 3/100\n",
      "259/259 [==============================] - 0s 100us/sample - loss: 19.0131 - acc: 0.4440\n",
      "Epoch 4/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 8.6363 - acc: 0.4402\n",
      "Epoch 5/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 3.2119 - acc: 0.5328\n",
      "Epoch 6/100\n",
      "259/259 [==============================] - 0s 97us/sample - loss: 3.6328 - acc: 0.6023\n",
      "Epoch 7/100\n",
      "259/259 [==============================] - 0s 89us/sample - loss: 2.8410 - acc: 0.5830\n",
      "Epoch 8/100\n",
      "259/259 [==============================] - 0s 100us/sample - loss: 2.4152 - acc: 0.5097\n",
      "Epoch 9/100\n",
      "259/259 [==============================] - 0s 127us/sample - loss: 2.2099 - acc: 0.5405\n",
      "Epoch 10/100\n",
      "259/259 [==============================] - 0s 112us/sample - loss: 2.0541 - acc: 0.5792\n",
      "Epoch 11/100\n",
      "259/259 [==============================] - 0s 143us/sample - loss: 1.8317 - acc: 0.5946\n",
      "Epoch 12/100\n",
      "259/259 [==============================] - 0s 124us/sample - loss: 1.5464 - acc: 0.5598\n",
      "Epoch 13/100\n",
      "259/259 [==============================] - 0s 97us/sample - loss: 1.3575 - acc: 0.6062\n",
      "Epoch 14/100\n",
      "259/259 [==============================] - 0s 97us/sample - loss: 1.1674 - acc: 0.6139\n",
      "Epoch 15/100\n",
      "259/259 [==============================] - ETA: 0s - loss: 1.0278 - acc: 0.718 - 0s 112us/sample - loss: 1.0818 - acc: 0.6062\n",
      "Epoch 16/100\n",
      "259/259 [==============================] - 0s 108us/sample - loss: 1.0128 - acc: 0.6139\n",
      "Epoch 17/100\n",
      "259/259 [==============================] - 0s 116us/sample - loss: 0.9635 - acc: 0.6409\n",
      "Epoch 18/100\n",
      "259/259 [==============================] - 0s 112us/sample - loss: 0.9292 - acc: 0.6602\n",
      "Epoch 19/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.9143 - acc: 0.6602\n",
      "Epoch 20/100\n",
      "259/259 [==============================] - 0s 96us/sample - loss: 0.9119 - acc: 0.6525\n",
      "Epoch 21/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.8705 - acc: 0.6718\n",
      "Epoch 22/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.9036 - acc: 0.6100\n",
      "Epoch 23/100\n",
      "259/259 [==============================] - 0s 89us/sample - loss: 0.8263 - acc: 0.6718\n",
      "Epoch 24/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.8552 - acc: 0.6641\n",
      "Epoch 25/100\n",
      "259/259 [==============================] - 0s 89us/sample - loss: 0.8261 - acc: 0.6332\n",
      "Epoch 26/100\n",
      "259/259 [==============================] - 0s 120us/sample - loss: 0.7946 - acc: 0.6757\n",
      "Epoch 27/100\n",
      "259/259 [==============================] - 0s 135us/sample - loss: 0.8156 - acc: 0.6448\n",
      "Epoch 28/100\n",
      "259/259 [==============================] - 0s 112us/sample - loss: 0.7773 - acc: 0.6873\n",
      "Epoch 29/100\n",
      "259/259 [==============================] - 0s 120us/sample - loss: 0.7598 - acc: 0.6757\n",
      "Epoch 30/100\n",
      "259/259 [==============================] - 0s 154us/sample - loss: 0.7376 - acc: 0.6988\n",
      "Epoch 31/100\n",
      "259/259 [==============================] - 0s 131us/sample - loss: 0.7522 - acc: 0.6641\n",
      "Epoch 32/100\n",
      "259/259 [==============================] - 0s 112us/sample - loss: 0.7259 - acc: 0.6834\n",
      "Epoch 33/100\n",
      "259/259 [==============================] - 0s 120us/sample - loss: 0.7730 - acc: 0.6873\n",
      "Epoch 34/100\n",
      "259/259 [==============================] - 0s 131us/sample - loss: 0.7535 - acc: 0.6641\n",
      "Epoch 35/100\n",
      "259/259 [==============================] - 0s 127us/sample - loss: 0.6792 - acc: 0.7027\n",
      "Epoch 36/100\n",
      "259/259 [==============================] - 0s 97us/sample - loss: 0.6700 - acc: 0.7181\n",
      "Epoch 37/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.6720 - acc: 0.7143\n",
      "Epoch 38/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.6412 - acc: 0.7181\n",
      "Epoch 39/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.6340 - acc: 0.7066\n",
      "Epoch 40/100\n",
      "259/259 [==============================] - 0s 74us/sample - loss: 0.6181 - acc: 0.7375\n",
      "Epoch 41/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.6243 - acc: 0.7104\n",
      "Epoch 42/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.5949 - acc: 0.7259\n",
      "Epoch 43/100\n",
      "259/259 [==============================] - 0s 70us/sample - loss: 0.5937 - acc: 0.7259\n",
      "Epoch 44/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.5926 - acc: 0.7066\n",
      "Epoch 45/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.5650 - acc: 0.7336\n",
      "Epoch 46/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.5563 - acc: 0.7375\n",
      "Epoch 47/100\n",
      "259/259 [==============================] - 0s 100us/sample - loss: 0.5809 - acc: 0.7104\n",
      "Epoch 48/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.5819 - acc: 0.7220\n",
      "Epoch 49/100\n",
      "259/259 [==============================] - 0s 104us/sample - loss: 0.5331 - acc: 0.7104\n",
      "Epoch 50/100\n",
      "259/259 [==============================] - 0s 104us/sample - loss: 0.5367 - acc: 0.7413\n",
      "Epoch 51/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.5367 - acc: 0.7568\n",
      "Epoch 52/100\n",
      "259/259 [==============================] - 0s 69us/sample - loss: 0.5439 - acc: 0.7104\n",
      "Epoch 53/100\n",
      "259/259 [==============================] - 0s 70us/sample - loss: 0.5069 - acc: 0.7568\n",
      "Epoch 54/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.5124 - acc: 0.7722\n",
      "Epoch 55/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.5169 - acc: 0.7645\n",
      "Epoch 56/100\n",
      "259/259 [==============================] - 0s 73us/sample - loss: 0.5078 - acc: 0.7490\n",
      "Epoch 57/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.5281 - acc: 0.7722\n",
      "Epoch 58/100\n",
      "259/259 [==============================] - 0s 236us/sample - loss: 0.5053 - acc: 0.7761\n",
      "Epoch 59/100\n",
      "259/259 [==============================] - 0s 127us/sample - loss: 0.5674 - acc: 0.7297\n",
      "Epoch 60/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.5069 - acc: 0.7683\n",
      "Epoch 61/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4852 - acc: 0.7568\n",
      "Epoch 62/100\n",
      "259/259 [==============================] - 0s 73us/sample - loss: 0.4763 - acc: 0.7568\n",
      "Epoch 63/100\n",
      "259/259 [==============================] - 0s 73us/sample - loss: 0.4687 - acc: 0.7645\n",
      "Epoch 64/100\n",
      "259/259 [==============================] - 0s 108us/sample - loss: 0.4710 - acc: 0.7838\n",
      "Epoch 65/100\n",
      "259/259 [==============================] - 0s 73us/sample - loss: 0.4677 - acc: 0.7838\n",
      "Epoch 66/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4923 - acc: 0.7722\n",
      "Epoch 67/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.4625 - acc: 0.7876\n",
      "Epoch 68/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.4692 - acc: 0.7915\n",
      "Epoch 69/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.4452 - acc: 0.7838\n",
      "Epoch 70/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.4553 - acc: 0.8147\n",
      "Epoch 71/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.4579 - acc: 0.7876\n",
      "Epoch 72/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.5259 - acc: 0.7413\n",
      "Epoch 73/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.5438 - acc: 0.7297\n",
      "Epoch 74/100\n",
      "259/259 [==============================] - 0s 89us/sample - loss: 0.5127 - acc: 0.7452\n",
      "Epoch 75/100\n",
      "259/259 [==============================] - 0s 89us/sample - loss: 0.4740 - acc: 0.7915\n",
      "Epoch 76/100\n",
      "259/259 [==============================] - 0s 78us/sample - loss: 0.4781 - acc: 0.7606\n",
      "Epoch 77/100\n",
      "259/259 [==============================] - 0s 112us/sample - loss: 0.4437 - acc: 0.7799\n",
      "Epoch 78/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.4599 - acc: 0.8031\n",
      "Epoch 79/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.4416 - acc: 0.8031\n",
      "Epoch 80/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.4790 - acc: 0.7683\n",
      "Epoch 81/100\n",
      "259/259 [==============================] - 0s 170us/sample - loss: 0.4891 - acc: 0.7452\n",
      "Epoch 82/100\n",
      "259/259 [==============================] - 0s 127us/sample - loss: 0.4376 - acc: 0.7954\n",
      "Epoch 83/100\n",
      "259/259 [==============================] - 0s 120us/sample - loss: 0.4323 - acc: 0.8108\n",
      "Epoch 84/100\n",
      "259/259 [==============================] - 0s 108us/sample - loss: 0.4306 - acc: 0.8031\n",
      "Epoch 85/100\n",
      "259/259 [==============================] - 0s 93us/sample - loss: 0.4293 - acc: 0.7954\n",
      "Epoch 86/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4899 - acc: 0.7722\n",
      "Epoch 87/100\n",
      "259/259 [==============================] - 0s 89us/sample - loss: 0.4413 - acc: 0.8069\n",
      "Epoch 88/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.4271 - acc: 0.8185\n",
      "Epoch 89/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4202 - acc: 0.8069\n",
      "Epoch 90/100\n",
      "259/259 [==============================] - 0s 81us/sample - loss: 0.4273 - acc: 0.8069\n",
      "Epoch 91/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4305 - acc: 0.8147\n",
      "Epoch 92/100\n",
      "259/259 [==============================] - 0s 97us/sample - loss: 0.4107 - acc: 0.8147\n",
      "Epoch 93/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.4164 - acc: 0.8031\n",
      "Epoch 94/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.4281 - acc: 0.8185\n",
      "Epoch 95/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4380 - acc: 0.7954\n",
      "Epoch 96/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4364 - acc: 0.7992\n",
      "Epoch 97/100\n",
      "259/259 [==============================] - 0s 73us/sample - loss: 0.4217 - acc: 0.7992\n",
      "Epoch 98/100\n",
      "259/259 [==============================] - 0s 85us/sample - loss: 0.4564 - acc: 0.7799\n",
      "Epoch 99/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4207 - acc: 0.8301\n",
      "Epoch 100/100\n",
      "259/259 [==============================] - 0s 77us/sample - loss: 0.4232 - acc: 0.8224\n"
     ]
    }
   ],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(Dense(16,input_dim=X_train.shape[1], activation='relu'))\n",
    "model0.add(Dense(8, activation='relu'))\n",
    "model0.add(Dense(1, activation='sigmoid'))\n",
    "model0.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist0 = model0.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 0.4232413315404796\n",
      "Final Accuracy: 82.23%\n"
     ]
    }
   ],
   "source": [
    "print('Final Loss: {}\\nFinal Accuracy: {}%'.format(hist0.history['loss'][-1:][0], str(hist0.history['acc'][-1:][0]*100)[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Hyperparameter = Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dylan nason\\anaconda3\\envs\\u4-s2-nn\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7992277992277992 using {'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.7992277992277992, Stdev: 0.04161996411727827 with: {'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.7258687258687259, Stdev: 0.02744314227121379 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.7335907335907336, Stdev: 0.019007133906083613 with: {'batch_size': 40, 'epochs': 100}\n",
      "Means: 0.637065637065637, Stdev: 0.11901872153997056 with: {'batch_size': 60, 'epochs': 100}\n",
      "Means: 0.5714285714285714, Stdev: 0.06769961087936796 with: {'batch_size': 80, 'epochs': 100}\n",
      "Means: 0.5135135135135135, Stdev: 0.12433751403045383 with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [100]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='accuracy')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal parameter setting: 'batch_size' : 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Hyperparameter = Optimizing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 12:20:56.124990 10872 deprecation.py:506] From c:\\users\\dylan nason\\anaconda3\\envs\\u4-s2-nn\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "c:\\users\\dylan nason\\anaconda3\\envs\\u4-s2-nn\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.806949806949807 using {'batch_size': 10, 'epochs': 100, 'optimizer': 'nadam'}\n",
      "Means: 0.7953667953667953, Stdev: 0.07577753641125404 with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'adam'}\n",
      "Means: 0.806949806949807, Stdev: 0.027858263048738216 with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'nadam'}\n",
      "Means: 0.555984555984556, Stdev: 0.012165344395952593 with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'sgd'}\n",
      "Means: 0.5444015444015444, Stdev: 0.05966726913076917 with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'rmsprop'}\n",
      "Means: 0.4517374517374517, Stdev: 0.025967893324604256 with: {'batch_size': 10, 'epochs': 100, 'optimizer': 'adagrad'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'batch_size': [10],\n",
    "              'epochs': [100],\n",
    "              'optimizer': ['adam', 'nadam', 'sgd', 'rmsprop', 'adagrad']}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='accuracy')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal parameter setting: 'optimizer': 'nadam'\n",
    "However, the optimizer 'adam' performed almost equally or above nadam when tested multiple times, so in this case \n",
    "the optimizers 'nadam' and 'adam' and pratically interchangable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Hyperparameter = loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(loss_func):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss=loss_func, optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dylan nason\\anaconda3\\envs\\u4-s2-nn\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.806949806949807 using {'batch_size': 10, 'epochs': 100, 'loss_func': 'binary_crossentropy'}\n",
      "Means: 0.806949806949807, Stdev: 0.03719490197599509 with: {'batch_size': 10, 'epochs': 100, 'loss_func': 'binary_crossentropy'}\n",
      "Means: 0.528957528957529, Stdev: 0.04943407357009829 with: {'batch_size': 10, 'epochs': 100, 'loss_func': 'hinge'}\n",
      "Means: 0.4826254826254826, Stdev: 0.05459296895262807 with: {'batch_size': 10, 'epochs': 100, 'loss_func': 'cosine_proximity'}\n",
      "Means: 0.49034749034749037, Stdev: 0.056472074253282274 with: {'batch_size': 10, 'epochs': 100, 'loss_func': 'categorical_hinge'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'batch_size': [10],\n",
    "              'epochs': [100],\n",
    "              'loss_func': ['binary_crossentropy', 'hinge', 'cosine_proximity', 'categorical_hinge']}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, scoring='accuracy')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal parameter setting: 'loss':'binary_crossentropy'"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
